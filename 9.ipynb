{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karube044/100/blob/master/9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjd7BWit4t42"
      },
      "source": [
        "# 80. ID番号への変換\n",
        "# 準備\n",
        "# 問50で作成した学習、検証、評価データを用いる\n",
        "import pandas as pd\n",
        "train = pd.read_csv('train.txt', header=0, sep='\\t')\n",
        "valid = pd.read_csv('valid.txt', header=0, sep='\\t')\n",
        "test = pd.read_csv('test.txt', header=0, sep='\\t')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZK3qzBwJTG_"
      },
      "source": [
        "# 単語のID番号を付与する\n",
        "import collections\n",
        "import string\n",
        "\n",
        "# 本文から記号を消去し、単語ごとに分けすべてリストに格納する\n",
        "table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "l = []\n",
        "for text in train['TITLE']:\n",
        "  words = text.translate(table).split()\n",
        "  for word in words:\n",
        "    l.append(word)\n",
        "\n",
        "# collections.Counterでリスト内の単語の出現頻度を辞書型として取得\n",
        "c = collections.Counter(l)\n",
        "# sorted()で辞書の要素を出現頻度の降順にリストとして取得\n",
        "f = sorted(c.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# リストの順番に順位(単語ID)をつけていく(2回以上出現した単語のみ)\n",
        "word_id={}\n",
        "for i, (w, cnt) in enumerate(f):\n",
        "  if cnt > 1:\n",
        "    word_id[w]= i+1 \n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmM_eDGG5CeM",
        "outputId": "1f16f7fd-187b-4386-d9ab-e6c037024430"
      },
      "source": [
        "# 確認(リストに変換し、先頭から10個目を表示する)\n",
        "tmp = list(word_id.items())\n",
        "print(tmp[:10])\n",
        "print(len(set(word_id.values())))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('to', 1), ('s', 2), ('in', 3), ('UPDATE', 4), ('on', 5), ('as', 6), ('US', 7), ('for', 8), ('of', 9), ('The', 10)]\n",
            "9397\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dts-87ZHaOzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d194da2d-3a50-4026-fce3-2f1049fd13ad"
      },
      "source": [
        "# 与えられた単語列に対し単語IDを返す関数\n",
        "def return_id(text):\n",
        "  # 単語列の単語IDを入れるリスト\n",
        "  ids = []\n",
        "  # 同じように文から単語を取得し、それに対応する単語IDをリストに与える\n",
        "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "  words = text.translate(table).split()\n",
        "  for word in words:\n",
        "    # get()で単語IDが与えられていない単語には0を返すようにする\n",
        "    ids.append(word_id.get(word, 0))\n",
        "\n",
        "  return ids\n",
        "\n",
        "# 確認(適当な記事見出しを選択)\n",
        "text = train['TITLE'][12]\n",
        "print(text)\n",
        "print(return_id(text))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "UPDATE 2-Dollar General CEO to retire, Icahn's proposed merger in doubt\n",
            "[4, 13, 55, 880, 60, 1, 6594, 2098, 2, 1568, 566, 3, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCwl1IUZic7H"
      },
      "source": [
        "# 81. RNNによる予測\n",
        "# 標準的に必要となるライブラリ\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHqHCNMlLDfg"
      },
      "source": [
        "# バッチ処理を行えるようにデータセットを作成する\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "  def __init__(self, xdata, ydata, return_id):\n",
        "    self.data = xdata\n",
        "    self.label = ydata\n",
        "    self.return_id = return_id\n",
        "\n",
        "  # len(Dataset)で返す値を指定\n",
        "  def __len__(self):  \n",
        "    return len(self.label)\n",
        "\n",
        "  # Dataset[idx]で返す値を指定\n",
        "  def __getitem__(self, idx):\n",
        "    # 前問で作成した関数を用いて単語列をIDに変換し返す\n",
        "    text = self.data[idx]\n",
        "    x = self.return_id(text)\n",
        "    y = self.label[idx]\n",
        "    return x,y\n",
        "\n",
        "# 文の長さが一定ではないのでPaddingを行ってDataLoaderの出力とする\n",
        "def my_collate_fn(batch):\n",
        "  # バッチサイズ分のデータを取得\n",
        "  xdata, ydata = list(zip(*batch))\n",
        "  xs = list(xdata)\n",
        "  # Paddingを行う\n",
        "  xs1 = []\n",
        "  for k in range(len(xs)):\n",
        "    ids = xs[k]\n",
        "    xs1.append(torch.LongTensor(ids))\n",
        "  # Paddingの数値は単語ID辞書に登録されている単語数+1とする\n",
        "  xs1 = pad_sequence(xs1, batch_first=True, padding_value=len(word_id)+1)\n",
        "  \n",
        "  ys = list(ydata)\n",
        "  ys1 = torch.LongTensor(ys)\n",
        "\n",
        "  return xs1, ys1"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Pw4WLLmc7ad"
      },
      "source": [
        "# 記事のカテゴリ名をラベルに変更するための辞書(8章と同じもの)\n",
        "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "Y_train = torch.tensor(list(map(lambda x: category_dict[x], train['CATEGORY'])))\n",
        "Y_valid = torch.tensor(list(map(lambda x: category_dict[x], valid['CATEGORY'])))\n",
        "Y_test = torch.tensor(list(map(lambda x: category_dict[x], test['CATEGORY'])))\n",
        "\n",
        "# それぞれのデータセットを作成\n",
        "dataset_train = MyDataset(train['TITLE'], Y_train, return_id)\n",
        "dataset_valid = MyDataset(valid['TITLE'], Y_valid, return_id)\n",
        "dataset_test = MyDataset(test['TITLE'], Y_test, return_id)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDUtKxHVORNw",
        "outputId": "59c3057b-8ed6-4234-e13d-69c252b0fcfe"
      },
      "source": [
        "# 確認(バッチ処理とするのでバッチサイズはデータ全て)\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=len(dataset_train), shuffle=True,\n",
        "                        collate_fn=my_collate_fn)\n",
        "d1 = dataloader_train.__iter__()\n",
        "xs, ys = d1.next()\n",
        "print(xs)\n",
        "print(len(xs))\n",
        "print(ys)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1569,   15,  945,  ..., 9398, 9398, 9398],\n",
            "        [ 155,    0, 3349,  ..., 9398, 9398, 9398],\n",
            "        [1386,   10, 6794,  ..., 9398, 9398, 9398],\n",
            "        ...,\n",
            "        [1257, 1985, 2747,  ..., 9398, 9398, 9398],\n",
            "        [3116,    2,  116,  ..., 9398, 9398, 9398],\n",
            "        [ 424,  601, 2778,  ..., 9398, 9398, 9398]])\n",
            "10672\n",
            "tensor([1, 3, 2,  ..., 2, 0, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIJTUVjXiwY2"
      },
      "source": [
        "# モデルの設定\n",
        "class Nlp81(nn.Module):\n",
        "  def __init__(self, vocab_size, embd_size, output_size, hid_size, padding_idx):\n",
        "    super(Nlp81, self).__init__()\n",
        "\n",
        "    # 埋め込み層でone-hotから単語ベクトルに変換\n",
        "    self.embd = nn.Embedding(vocab_size, embd_size, padding_idx=padding_idx)\n",
        "\n",
        "    # RNN層を定義\n",
        "    # 活性化関数はtanhとする   \n",
        "    self.rnn = nn.RNN(embd_size, hid_size, nonlinearity='tanh', batch_first=True)\n",
        "\n",
        "    # 隠れ層を定義\n",
        "    self.l1=nn.Linear(hid_size, output_size)\n",
        "    # 今回は重みをランダムな値で(正規分布により)初期化する\n",
        "    nn.init.normal_(self.l1.weight, 0.0, 1.0) \n",
        "        \n",
        "  # 順方向の計算\n",
        "  def forward(self,x):\n",
        "    # 埋め込み層で単語ベクトルに直し\n",
        "    ex = self.embd(x)\n",
        "    # RNNユニットで計算する\n",
        "    out, h_T = self.rnn(ex)\n",
        "\n",
        "    # RNNでは各単語に対する出力と、最後の隠れ状態ベクトルが得られる\n",
        "    # 最後の隠れ状態ベクトルは単語列の最後の単語に対する出力と等しいのでそちらを使用する\n",
        "    # ソフトマックス関数を適用し求める(dim=-1で行のsoftmaxを指定)\n",
        "    h1 = torch.softmax(self.l1(out[:,-1,:]), dim=-1)\n",
        "    return h1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcIVyDd0SaTJ",
        "outputId": "8d0c1d37-14e4-496e-e2ac-1ea96941e623"
      },
      "source": [
        "# 確認\n",
        "# モデルの引数は単語の総数、単語ベクトル、出力ラベル、隠れ層の次元、Paddingの数値\n",
        "# 単語の総数は単語ID辞書に登録されている単語数＋2(登録されていない単語+Padding)\n",
        "model = Nlp81(len(word_id)+2, 300, 4, 50, len(word_id)+1)\n",
        "for xs, ys in dataloader_train:\n",
        "  y1_hat=model(xs)\n",
        "  print(y1_hat)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0287, 0.2458, 0.4089, 0.3166],\n",
            "        [0.0286, 0.2487, 0.4042, 0.3185],\n",
            "        [0.0264, 0.2724, 0.4065, 0.2947],\n",
            "        ...,\n",
            "        [0.0283, 0.2489, 0.4030, 0.3198],\n",
            "        [0.0283, 0.2462, 0.4077, 0.3178],\n",
            "        [0.0285, 0.2457, 0.4122, 0.3137]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK1lvDS50f5y"
      },
      "source": [
        "# 82. 確率的勾配降下法による学習\n",
        "\n",
        "# 正解率と損失の計算のための関数\n",
        "def cal_loss_accuracy(model, criterion, dataset):\n",
        "  # 正解率、損失を求めるときには比較のためshuffleはFalseとする\n",
        "  # バッチ処理で求める\n",
        "  dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False,\n",
        "                        collate_fn=my_collate_fn)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for xs, ys in dataloader:\n",
        "      # 順方向による出力\n",
        "      output = model(xs)\n",
        "      # 損失の計算\n",
        "      loss = criterion(output, ys).item()\n",
        "      # 正解率の計算\n",
        "      ans = torch.argmax(output,1)\n",
        "      accuracy = ((ys == ans).sum().float() / len(ans)).item()\n",
        "  return loss, accuracy"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOj__PRDDGQS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a2ca5a-2442-4b0b-cf65-967599bf1287"
      },
      "source": [
        "# モデルのインスタンス、損失関数、最適化関数の設定\n",
        "model = Nlp81(len(word_id)+2, 300, 4, 50, len(word_id)+1)\n",
        "\n",
        "# ラベルはPaddingを行っていないのでクロスエントロピーで気にする必要はない\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# データをロードする\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=len(dataset_train), shuffle=True,\n",
        "                        collate_fn=my_collate_fn)\n",
        "\n",
        "# 学習の開始\n",
        "# 10エポックで終了とする\n",
        "for ep in range(10):\n",
        "  \n",
        "  model.train()\n",
        "  # データも81で取得したdataloader_trainを使用する\n",
        "  for xs, ys in dataloader_train:\n",
        "    output = model(xs) \n",
        "    loss = criterion(output, ys) \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() \n",
        "    optimizer.step()\n",
        "\n",
        "  loss_train, accuracy_train = cal_loss_accuracy(model, criterion, dataset_train)\n",
        "  loss_test, accuracy_test = cal_loss_accuracy(model, criterion, dataset_test) \n",
        "  print('エポック数', ep)\n",
        "  print('[訓練データ]損失：',loss_train,'正解率:',accuracy_train)\n",
        "  print('[評価データ]損失：',loss_test,'正解率:',accuracy_test)\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "エポック数 0\n",
            "[訓練データ]損失： 1.2951695919036865 正解率: 0.4222263991832733\n",
            "[評価データ]損失： 1.294743537902832 正解率: 0.4220389723777771\n",
            "エポック数 1\n",
            "[訓練データ]損失： 1.283414602279663 正解率: 0.42306971549987793\n",
            "[評価データ]損失： 1.2827931642532349 正解率: 0.4220389723777771\n",
            "エポック数 2\n",
            "[訓練データ]損失： 1.2742602825164795 正解率: 0.4253185987472534\n",
            "[評価データ]損失： 1.2733274698257446 正解率: 0.4220389723777771\n",
            "エポック数 3\n",
            "[訓練データ]損失： 1.2694734334945679 正解率: 0.4300037622451782\n",
            "[評価データ]損失： 1.2684270143508911 正解率: 0.4220389723777771\n",
            "エポック数 4\n",
            "[訓練データ]損失： 1.2669267654418945 正解率: 0.4315029978752136\n",
            "[評価データ]損失： 1.2659424543380737 正解率: 0.4220389723777771\n",
            "エポック数 5\n",
            "[訓練データ]損失： 1.2652689218521118 正解率: 0.4320652186870575\n",
            "[評価データ]損失： 1.2643778324127197 正解率: 0.4220389723777771\n",
            "エポック数 6\n",
            "[訓練データ]損失： 1.264090657234192 正解率: 0.43234631419181824\n",
            "[評価データ]損失： 1.2632941007614136 正解率: 0.4220389723777771\n",
            "エポック数 7\n",
            "[訓練データ]損失： 1.2632015943527222 正解率: 0.43318966031074524\n",
            "[評価データ]損失： 1.2625010013580322 正解率: 0.4220389723777771\n",
            "エポック数 8\n",
            "[訓練データ]損失： 1.2624998092651367 正解率: 0.4330959618091583\n",
            "[評価データ]損失： 1.2618964910507202 正解率: 0.4220389723777771\n",
            "エポック数 9\n",
            "[訓練データ]損失： 1.2619256973266602 正解率: 0.4335644543170929\n",
            "[評価データ]損失： 1.2614208459854126 正解率: 0.4220389723777771\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JkfDoLBTZFd"
      },
      "source": [
        "# 83. ミニバッチ化・GPU上での学習\n",
        "# GPUに送るためのコード\n",
        "device = torch.device(\"cuda:0\" \n",
        "                     if torch.cuda.is_available()\n",
        "                     else \"cpu\") "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N92sZeLCTZLz",
        "outputId": "cd7e5488-5a66-473e-8c34-99fd70793ae3"
      },
      "source": [
        "# バッチサイズは、データロードのときにサイズを指定すれば変更できる\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=4, shuffle=True,\n",
        "                        collate_fn=my_collate_fn)\n",
        "\n",
        "# 確認\n",
        "d1 = dataloader_train.__iter__()\n",
        "xs, ys = d1.next()\n",
        "print(xs)\n",
        "print(len(xs))\n",
        "print(ys)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 127,   17,  168,  978,   52,    2,   84,  113,   25,   20, 8183, 9398],\n",
            "        [  51,  402,    5, 4759,   88,  484,    6,   67, 3971, 5246,    8,  830],\n",
            "        [  22,   19,   34,   30,  587, 1655,    5,   14, 1591, 9398, 9398, 9398],\n",
            "        [ 404,   60,   36,  760,  306,  476,  108,  217, 7129, 1347,    3,  481]])\n",
            "4\n",
            "tensor([2, 0, 2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRwNA85HWdtB"
      },
      "source": [
        "# モデルの設定(81の流用(重みなどの初期化は省いている))\n",
        "class Nlp83(nn.Module):\n",
        "  def __init__(self, vocab_size, embd_size, output_size, hid_size, padding_idx):\n",
        "    super(Nlp83, self).__init__()\n",
        "\n",
        "    self.embd = nn.Embedding(vocab_size, embd_size, padding_idx=padding_idx)\n",
        "    self.rnn = nn.RNN(embd_size, hid_size, nonlinearity='tanh', batch_first=True)\n",
        "    self.l1=nn.Linear(hid_size, output_size)\n",
        "        \n",
        "  def forward(self,x):\n",
        "    ex = self.embd(x)\n",
        "    out, h_T = self.rnn(ex)\n",
        "    h1 = torch.softmax(self.l1(out[:,-1,:]), dim=-1)\n",
        "    return h1"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsPiqgpEXcdK"
      },
      "source": [
        "# 正解率と損失の計算のための関数(GPU用)\n",
        "def cal_loss_accuracy_gpu(model, criterion, dataset):\n",
        "  dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=False,\n",
        "                        collate_fn=my_collate_fn)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for xs, ys in dataloader:\n",
        "      # 計算を行うためのベクトルをGPUに移動する\n",
        "      xs = xs.to(device)\n",
        "      ys = ys.to(device)\n",
        "      output = model(xs)\n",
        "      loss = criterion(output, ys).item()\n",
        "      ans = torch.argmax(output,1)\n",
        "      accuracy = ((ys == ans).sum().float() / len(ans)).item()\n",
        "  return loss, accuracy"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIfYZY0gTZRT",
        "outputId": "988deff3-d1a0-456a-c904-819835de6a4a"
      },
      "source": [
        "# モデルのインスタンス、損失関数、最適化関数の設定\n",
        "# モデルのインスタンス作成時にGPUに送る\n",
        "model = Nlp83(len(word_id)+2, 300, 4, 50, len(word_id)+1).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# データロード時にバッチサイズを指定する(今回は4とする)\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=4, shuffle=True,\n",
        "                        collate_fn=my_collate_fn)\n",
        "\n",
        "# 学習の開始\n",
        "# 10エポックで終了とする\n",
        "for ep in range(10):\n",
        "  model.train()\n",
        "  for xs, ys in dataloader_train:\n",
        "    # 学習時に使用するベクトルをGPUに移動する\n",
        "    xs = xs.to(device)\n",
        "    ys = ys.to(device)\n",
        "    output = model(xs) \n",
        "    loss = criterion(output, ys) \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() \n",
        "    optimizer.step()\n",
        "\n",
        "  loss_train, accuracy_train = cal_loss_accuracy_gpu(model, criterion, dataset_train)\n",
        "  loss_test, accuracy_test = cal_loss_accuracy_gpu(model, criterion, dataset_test) \n",
        "  print('エポック数', ep)\n",
        "  print('[訓練データ]損失：',loss_train,'正解率:',accuracy_train)\n",
        "  print('[評価データ]損失：',loss_test,'正解率:',accuracy_test)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "エポック数 0\n",
            "[訓練データ]損失： 1.300644040107727 正解率: 0.42185157537460327\n",
            "[評価データ]損失： 1.3006950616836548 正解率: 0.4220389723777771\n",
            "エポック数 1\n",
            "[訓練データ]損失： 1.2570574283599854 正解率: 0.44865068793296814\n",
            "[評価データ]損失： 1.2578916549682617 正解率: 0.4227886199951172\n",
            "エポック数 2\n",
            "[訓練データ]損失： 1.163278341293335 正解率: 0.5803973078727722\n",
            "[評価データ]損失： 1.1754601001739502 正解率: 0.5682159066200256\n",
            "エポック数 3\n",
            "[訓練データ]損失： 1.286340594291687 正解率: 0.45239880681037903\n",
            "[評価データ]損失： 1.3207647800445557 正解率: 0.4227886199951172\n",
            "エポック数 4\n",
            "[訓練データ]損失： 1.2519757747650146 正解率: 0.4906296730041504\n",
            "[評価データ]損失： 1.278905987739563 正解率: 0.46476760506629944\n",
            "エポック数 5\n",
            "[訓練データ]損失： 1.165626883506775 正解率: 0.577867329120636\n",
            "[評価データ]損失： 1.188205599784851 正解率: 0.5554722547531128\n",
            "エポック数 6\n",
            "[訓練データ]損失： 1.1831978559494019 正解率: 0.5528485774993896\n",
            "[評価データ]損失： 1.2030823230743408 正解率: 0.5367316603660583\n",
            "エポック数 7\n",
            "[訓練データ]損失： 1.1659795045852661 正解率: 0.5776799321174622\n",
            "[評価データ]損失： 1.2001930475234985 正解率: 0.54347825050354\n",
            "エポック数 8\n",
            "[訓練データ]損失： 1.1475555896759033 正解率: 0.596045732498169\n",
            "[評価データ]損失： 1.1934453248977661 正解率: 0.5502249002456665\n",
            "エポック数 9\n",
            "[訓練データ]損失： 1.143967866897583 正解率: 0.5995127558708191\n",
            "[評価データ]損失： 1.1492220163345337 正解率: 0.5944527983665466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F41DoC2wZv1P"
      },
      "source": [
        "# 84. 単語ベクトルの導入\n",
        "# 学習済み単語ベクトルのダウンロード(コピペ)\n",
        "# 学習済み単語ベクトルのダウンロード\n",
        "FILE_ID = \"0B7XkCwpI5KDYNlNUTTlSS21pQmM\"\n",
        "FILE_NAME = \"GoogleNews-vectors-negative300.bin.gz\"\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=$FILE_ID' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=$FILE_ID\" -O $FILE_NAME && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv_IEGnHhvUb"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "embd_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFVn7MxIfjnU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "303856da-5369-4140-89d2-25a46a605eb0"
      },
      "source": [
        "# 事前学習済みの単語ベクトルで初期化するための行列を作成する\n",
        "# 行列のサイズを決める(単語数、単語ベクトルの次元)\n",
        "vocab_size = len(word_id)+2\n",
        "embd_size = 300\n",
        "# そのサイズの0行列を作成\n",
        "embd_weights = np.zeros((vocab_size, embd_size))\n",
        "# 単語ID辞書に登録されている単語が事前学習済みの単語ベクトルに存在するなら\n",
        "# その単語ベクトルを取得する\n",
        "for i, word  in enumerate(word_id.keys()):\n",
        "    if word in embd_model.index2word:\n",
        "        embd_weights[i] = embd_model[word]\n",
        "\n",
        "# 配列の型が異なるので合わせておく  \n",
        "embd_weights = torch.from_numpy(embd_weights.astype((np.float32)))\n",
        "# 確認\n",
        "print(embd_weights)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.2910,  0.1787,  0.0500,  ..., -0.0228,  0.1177,  0.3535],\n",
            "        [ 0.0703,  0.0869,  0.0879,  ..., -0.0476,  0.0145, -0.0625],\n",
            "        ...,\n",
            "        [ 0.2207,  0.1963,  0.0649,  ...,  0.1011,  0.0388, -0.1152],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i47hTJK3jnZi"
      },
      "source": [
        "# モデルの設定(81の流用)\n",
        "class Nlp84(nn.Module):\n",
        "  def __init__(self, vocab_size, embd_size, output_size, hid_size, padding_idx, embd_weights):\n",
        "    super(Nlp84, self).__init__()\n",
        "\n",
        "    # 事前学習済み単語ベクトルを使う場合は\n",
        "    # Embedding.from_pretrainedとし、パラメータに作成した行列を指定する\n",
        "    self.embd = nn.Embedding.from_pretrained(embd_weights, padding_idx=padding_idx)\n",
        "    self.rnn = nn.RNN(embd_size, hid_size, nonlinearity='tanh', batch_first=True)\n",
        "    self.l1=nn.Linear(hid_size, output_size)\n",
        "        \n",
        "  def forward(self,x):\n",
        "    ex = self.embd(x)\n",
        "    out, h_T = self.rnn(ex)\n",
        "    h1 = torch.softmax(self.l1(out[:,-1,:]), dim=-1)\n",
        "    return h1"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wVZt70ujdQY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5567ff7f-1169-4ffe-b586-a7bbc4c4a1b5"
      },
      "source": [
        "# モデルのインスタンス、損失関数、最適化関数の設定(GPUには送らない)\n",
        "# 引数に作成した行列を追加する\n",
        "model = Nlp84(len(word_id)+2, 300, 4, 50, len(word_id)+1, embd_weights)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# 今回もバッチサイズ4とする\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=4, shuffle=True,\n",
        "                        collate_fn=my_collate_fn)\n",
        "\n",
        "# 学習の開始\n",
        "# 10エポックで終了とする\n",
        "for ep in range(10):\n",
        "  model.train()\n",
        "  for xs, ys in dataloader_train:\n",
        "    output = model(xs) \n",
        "    loss = criterion(output, ys) \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() \n",
        "    optimizer.step()\n",
        "\n",
        "  loss_train, accuracy_train = cal_loss_accuracy(model, criterion, dataset_train)\n",
        "  loss_test, accuracy_test = cal_loss_accuracy(model, criterion, dataset_test) \n",
        "  print('エポック数', ep)\n",
        "  print('[訓練データ]損失：',loss_train,'正解率:',accuracy_train)\n",
        "  print('[評価データ]損失：',loss_test,'正解率:',accuracy_test)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "エポック数 0\n",
            "[訓練データ]損失： 1.2789360284805298 正解率: 0.43412667512893677\n",
            "[評価データ]損失： 1.2973862886428833 正解率: 0.32383808493614197\n",
            "エポック数 1\n",
            "[訓練データ]損失： 1.2983262538909912 正解率: 0.42185157537460327\n",
            "[評価データ]損失： 1.2979673147201538 正解率: 0.4220389723777771\n",
            "エポック数 2\n",
            "[訓練データ]損失： 1.319261908531189 正解率: 0.3957083821296692\n",
            "[評価データ]損失： 1.3188124895095825 正解率: 0.3958021104335785\n",
            "エポック数 3\n",
            "[訓練データ]損失： 1.2765278816223145 正解率: 0.39683282375335693\n",
            "[評価データ]損失： 1.2771085500717163 正解率: 0.3965517282485962\n",
            "エポック数 4\n",
            "[訓練データ]損失： 1.3178727626800537 正解率: 0.4196026921272278\n",
            "[評価データ]損失： 1.319366455078125 正解率: 0.4227886199951172\n",
            "エポック数 5\n",
            "[訓練データ]損失： 1.2825437784194946 正解率: 0.4090142548084259\n",
            "[評価データ]損失： 1.2890218496322632 正解率: 0.4227886199951172\n",
            "エポック数 6\n",
            "[訓練データ]損失： 1.289052128791809 正解率: 0.42185157537460327\n",
            "[評価データ]損失： 1.2888617515563965 正解率: 0.4220389723777771\n",
            "エポック数 7\n",
            "[訓練データ]損失： 1.2895276546478271 正解率: 0.42185157537460327\n",
            "[評価データ]損失： 1.289247751235962 正解率: 0.4220389723777771\n",
            "エポック数 8\n",
            "[訓練データ]損失： 1.3198267221450806 正解率: 0.3957083821296692\n",
            "[評価データ]損失： 1.3196851015090942 正解率: 0.3958021104335785\n",
            "エポック数 9\n",
            "[訓練データ]損失： 1.268262267112732 正解率: 0.42185157537460327\n",
            "[評価データ]損失： 1.2676981687545776 正解率: 0.4227886199951172\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETRidP4urpmW"
      },
      "source": [
        "# 85. 双方向RNN・多層化\n",
        "\n",
        "# 双方向RNN・多層化はnn.RNNのパラメータを変更するだけで実装可能\n",
        "# モデルの設定(81の流用(事前学習済み単語ベクトルでは評価が下がったので使用しない))\n",
        "class Nlp85(nn.Module):\n",
        "  def __init__(self, vocab_size, embd_size, output_size, hid_size, padding_idx):\n",
        "    super(Nlp85, self).__init__()\n",
        "    self.embd = nn.Embedding(vocab_size, embd_size, padding_idx=padding_idx)\n",
        "\n",
        "    # 双方向にするにはbidirectional=True\n",
        "    # 多層化にはnum_layersの数値を変更すればよい(デフォルトは1)\n",
        "    self.rnn = nn.RNN(embd_size, hid_size, nonlinearity='tanh', batch_first=True,\n",
        "                      num_layers=2, bidirectional=True)\n",
        "    \n",
        "    # この時RNNのあとの分散表現のサイズが2倍になっているので注意\n",
        "    self.l1=nn.Linear(hid_size*2, output_size)\n",
        "        \n",
        "  def forward(self,x):\n",
        "    ex = self.embd(x)\n",
        "    out, h_T = self.rnn(ex)\n",
        "    h1 = torch.softmax(self.l1(out[:,-1,:]), dim=-1)\n",
        "    return h1"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMhB71LNsgIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d48bbc6-bf9a-4a7b-c001-c500c5e8a4f1"
      },
      "source": [
        "# モデルのインスタンス、損失関数、最適化関数の設定\n",
        "model = Nlp85(len(word_id)+2, 300, 4, 50, len(word_id)+1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=4, shuffle=True,\n",
        "                        collate_fn=my_collate_fn)\n",
        "\n",
        "# 学習の開始\n",
        "# 10エポックで終了とする\n",
        "for ep in range(10):\n",
        "  model.train()\n",
        "  for xs, ys in dataloader_train:\n",
        "    output = model(xs) \n",
        "    loss = criterion(output, ys) \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() \n",
        "    optimizer.step()\n",
        "\n",
        "  loss_train, accuracy_train = cal_loss_accuracy(model, criterion, dataset_train)\n",
        "  loss_test, accuracy_test = cal_loss_accuracy(model, criterion, dataset_test) \n",
        "  print('エポック数', ep)\n",
        "  print('[訓練データ]損失：',loss_train,'正解率:',accuracy_train)\n",
        "  print('[評価データ]損失：',loss_test,'正解率:',accuracy_test)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "エポック数 0\n",
            "[訓練データ]損失： 1.2618197202682495 正解率: 0.422132670879364\n",
            "[評価データ]損失： 1.2628974914550781 正解率: 0.4227886199951172\n",
            "エポック数 1\n",
            "[訓練データ]損失： 1.1313163042068481 正解率: 0.6144115328788757\n",
            "[評価データ]損失： 1.3472105264663696 正解率: 0.3958021104335785\n",
            "エポック数 2\n",
            "[訓練データ]損失： 1.0827618837356567 正解率: 0.6619190573692322\n",
            "[評価データ]損失： 1.093271017074585 正解率: 0.6514242887496948\n",
            "エポック数 3\n",
            "[訓練データ]損失： 1.1246137619018555 正解率: 0.6196589469909668\n",
            "[評価データ]損失： 1.1422545909881592 正解率: 0.6019490361213684\n",
            "エポック数 4\n",
            "[訓練データ]損失： 1.3479722738265991 正解率: 0.3957083821296692\n",
            "[評価データ]損失： 1.348515510559082 正解率: 0.3950524628162384\n",
            "エポック数 5\n",
            "[訓練データ]損失： 1.3478336334228516 正解率: 0.3957083821296692\n",
            "[評価データ]損失： 1.3480664491653442 正解率: 0.3958021104335785\n",
            "エポック数 6\n",
            "[訓練データ]損失： 1.3193633556365967 正解率: 0.4242878556251526\n",
            "[評価データ]損失： 1.3209311962127686 正解率: 0.4227886199951172\n",
            "エポック数 7\n",
            "[訓練データ]損失： 1.1567015647888184 正解率: 0.5718703269958496\n",
            "[評価データ]損失： 1.1563701629638672 正解率: 0.5787106156349182\n",
            "エポック数 8\n",
            "[訓練データ]損失： 1.2206745147705078 正解率: 0.5223950743675232\n",
            "[評価データ]損失： 1.245054006576538 正解率: 0.4985007643699646\n",
            "エポック数 9\n",
            "[訓練データ]損失： 1.2083868980407715 正解率: 0.5355135202407837\n",
            "[評価データ]損失： 1.2169965505599976 正解率: 0.5269864797592163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpZDgOU65Wg-"
      },
      "source": [
        "# 86. 畳み込みニューラルネットワーク (CNN)\n",
        "# モデルの作成\n",
        "class Nlp86(nn.Module):\n",
        "  def __init__(self, vocab_size, embd_size, output_size, hid_size, padding_idx, kernel_size, stride, padding):\n",
        "    super(Nlp86, self).__init__()\n",
        "    self.embd = nn.Embedding(vocab_size, embd_size, padding_idx=padding_idx)\n",
        "\n",
        "    # CNNはPyTorchのConv2dを用いる\n",
        "    # kernel_size(フィルターのサイズ)、ストライド、パディングの有無(オプションは0)を決定\n",
        "    # 最初の１はCNNの層の数（おそらく）\n",
        "    self.cnn = nn.Conv2d(1, hid_size, (kernel_size, embd_size), stride, (padding, 0))\n",
        "    # 活性化関数は今回はreluを用いる\n",
        "    self.g = nn.ReLU()\n",
        "    self.l1=nn.Linear(hid_size, output_size)\n",
        "        \n",
        "  def forward(self,x):\n",
        "    # 埋め込み層に通した後、unsqueezeでバッチとして直す(CNNに通すため)\n",
        "    ex = self.embd(x).unsqueeze(1)\n",
        "    cn = self.cnn(ex)\n",
        "    # 3個のトークンを活性化関数(今回はrelu)に入力する\n",
        "    pt = self.g(cn.squeeze(3))\n",
        "    # max_pool1d(一次元)を用いて最大値プーリングを行う\n",
        "    # pt.size()[2]はその文の単語数を表している\n",
        "    # つまり文全体を範囲としてプーリングを行う\n",
        "    c = F.max_pool1d(pt, pt.size()[2])\n",
        "    # プーリングで得たcの要素数は3つあるが、最後の軸が必要ないので、squeezeで省く\n",
        "    h1 = torch.softmax(self.l1(c.squeeze(2)), dim=-1)\n",
        "    return h1"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScndJKjr5Y4Z",
        "outputId": "d22ca72d-738f-4059-e71c-91e5802be8a1"
      },
      "source": [
        "# 確認\n",
        "# モデルの引数はRNNとほぼ同じで、最後の3つがフィルターのサイズ(3)、ストライド(1)、パディングの有無(有)\n",
        "model = Nlp86(len(word_id)+2, 300, 4, 50, len(word_id)+1, 3, 1, 1)\n",
        "\n",
        "# 今回は確認なのでバッチ処理で行う\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=len(dataset_train), shuffle=True,\n",
        "                        collate_fn=my_collate_fn)\n",
        "for xs, ys in dataloader_train:\n",
        "  y1_hat=model(xs)\n",
        "  print(y1_hat)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2035, 0.1997, 0.3897, 0.2071],\n",
            "        [0.2001, 0.1596, 0.4919, 0.1484],\n",
            "        [0.2259, 0.1484, 0.4345, 0.1913],\n",
            "        ...,\n",
            "        [0.1860, 0.0986, 0.4852, 0.2302],\n",
            "        [0.1838, 0.1343, 0.5296, 0.1523],\n",
            "        [0.2429, 0.1552, 0.4276, 0.1743]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2eTUYeiE6z2",
        "outputId": "79dac6a2-8497-4ef9-dda8-edc01727d9ad"
      },
      "source": [
        "# 87. 確率的勾配降下法によるCNNの学習\n",
        "# RNNのときと変わりはない\n",
        "# モデルのインスタンス、損失関数、最適化関数の設定\n",
        "model = Nlp86(len(word_id)+2, 300, 4, 50, len(word_id)+1, 3, 1, 1)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# バッチサイズは同じく4とする\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=4, shuffle=True,\n",
        "                        collate_fn=my_collate_fn)\n",
        "\n",
        "# 学習の開始\n",
        "# 10エポックで終了とする\n",
        "for ep in range(10):\n",
        "  model.train()\n",
        "  for xs, ys in dataloader_train:\n",
        "    output = model(xs) \n",
        "    loss = criterion(output, ys) \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() \n",
        "    optimizer.step()\n",
        "\n",
        "  loss_train, accuracy_train = cal_loss_accuracy(model, criterion, dataset_train)\n",
        "  loss_test, accuracy_test = cal_loss_accuracy(model, criterion, dataset_test) \n",
        "  print('エポック数', ep)\n",
        "  print('[訓練データ]損失：',loss_train,'正解率:',accuracy_train)\n",
        "  print('[評価データ]損失：',loss_test,'正解率:',accuracy_test)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "エポック数 0\n",
            "[訓練データ]損失： 0.99582839012146 正解率: 0.7579647898674011\n",
            "[評価データ]損失： 1.0292918682098389 正解率: 0.7181409001350403\n",
            "エポック数 1\n",
            "[訓練データ]損失： 0.956272542476654 正解率: 0.7919790148735046\n",
            "[評価データ]損失： 1.0095410346984863 正解率: 0.739130437374115\n",
            "エポック数 2\n",
            "[訓練データ]損失： 0.9396811723709106 正解率: 0.8038793206214905\n",
            "[評価データ]損失： 0.9952110052108765 正解率: 0.7473763227462769\n",
            "エポック数 3\n",
            "[訓練データ]損失： 0.9321897625923157 正解率: 0.8094077706336975\n",
            "[評価データ]損失： 0.9941712021827698 正解率: 0.7481259107589722\n",
            "エポック数 4\n",
            "[訓練データ]損失： 0.9273912310600281 正解率: 0.8118440508842468\n",
            "[評価データ]損失： 0.9898421764373779 正解率: 0.7481259107589722\n",
            "エポック数 5\n",
            "[訓練データ]損失： 0.9249825477600098 正解率: 0.8130621910095215\n",
            "[評価データ]損失： 0.9882882833480835 正解率: 0.7533733248710632\n",
            "エポック数 6\n",
            "[訓練データ]損失： 0.9230136275291443 正解率: 0.8139992356300354\n",
            "[評価データ]損失： 0.9866572022438049 正解率: 0.7533733248710632\n",
            "エポック数 7\n",
            "[訓練データ]損失： 0.9217024445533752 正解率: 0.8145614862442017\n",
            "[評価データ]損失： 0.9871091246604919 正解率: 0.7503747940063477\n",
            "エポック数 8\n",
            "[訓練データ]損失： 0.9208493828773499 正解率: 0.8149362802505493\n",
            "[評価データ]損失： 0.9866696000099182 正解率: 0.7541229128837585\n",
            "エポック数 9\n",
            "[訓練データ]損失： 0.9198656678199768 正解率: 0.815311074256897\n",
            "[評価データ]損失： 0.9862561821937561 正解率: 0.7533733248710632\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e99yVMPwWx_M"
      },
      "source": [
        "# 88. パラメータチューニング\n",
        "# チューニングのためにoptunaを用いるのでインストールする\n",
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKKEx5lNY7NE"
      },
      "source": [
        "# 85(双方向RNN・多層化)のモデル\n",
        "# 活性化関数をtanhとreluに変更できるようにする\n",
        "class RNN88(nn.Module):\n",
        "  def __init__(self, vocab_size, embd_size, output_size, hid_size, padding_idx, g):\n",
        "    super(RNN88, self).__init__()\n",
        "    self.embd = nn.Embedding(vocab_size, embd_size, padding_idx=padding_idx)\n",
        "    if g == 'tanh':\n",
        "      self.rnn = nn.RNN(embd_size, hid_size, nonlinearity='tanh', batch_first=True,\n",
        "                        num_layers=2, bidirectional=True)\n",
        "    elif g == 'relu':\n",
        "      self.rnn = nn.RNN(embd_size, hid_size, nonlinearity='relu', batch_first=True,\n",
        "                        num_layers=2, bidirectional=True)\n",
        "    self.l1=nn.Linear(hid_size*2, output_size)\n",
        "        \n",
        "  def forward(self,x):\n",
        "    ex = self.embd(x)\n",
        "    out, h_T = self.rnn(ex)\n",
        "    h1 = torch.softmax(self.l1(out[:,-1,:]), dim=-1)\n",
        "    return h1"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Imhl-HRRYeRA"
      },
      "source": [
        "# 86(CNN)のモデル\n",
        "class CNN88(nn.Module):\n",
        "  def __init__(self, vocab_size, embd_size, output_size, hid_size, padding_idx, kernel_size, stride, padding, g):\n",
        "    super(CNN88, self).__init__()\n",
        "    self.embd = nn.Embedding(vocab_size, embd_size, padding_idx=padding_idx)\n",
        "    self.cnn = nn.Conv2d(1, hid_size, (kernel_size, embd_size), stride, (padding, 0))\n",
        "    if g == 'tanh':\n",
        "      self.g = nn.Tanh()\n",
        "    elif g == 'relu':\n",
        "      self.g = nn.ReLU()\n",
        "    self.l1=nn.Linear(hid_size, output_size)\n",
        "        \n",
        "  def forward(self,x):\n",
        "    ex = self.embd(x).unsqueeze(1)\n",
        "    cn = self.cnn(ex)\n",
        "    pt = self.g(cn.squeeze(3))\n",
        "    c = F.max_pool1d(pt, pt.size()[2])\n",
        "    h1 = torch.softmax(self.l1(c.squeeze(2)), dim=-1)\n",
        "    return h1"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkvBge1FW0FL"
      },
      "source": [
        "# optunaを用いてパラメータの探索を行う\n",
        "import optuna\n",
        "# 探索のための関数の設定\n",
        "def objective(trial):\n",
        "  # チューニング対象のパラメータを決める\n",
        "  # 用いるモデル、埋め込み層のサイズ、隠れ層のサイズ、学習率、バッチサイズを対象とする\n",
        "  model_category = trial.suggest_categorical(\"category\", [\"RNN\",\"CNN\"])\n",
        "  embd_size = int(trial.suggest_discrete_uniform('embd_size', 100, 400, 100))\n",
        "  hid_size = int(trial.suggest_discrete_uniform('hid_size', 50, 200, 50))\n",
        "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-3, 1e-1)\n",
        "  batch_size = int(trial.suggest_discrete_uniform('batch_size', 4, 16, 4))\n",
        "  g = trial.suggest_categorical(\"g\", [\"tanh\", \"relu\"])\n",
        "\n",
        "  # 単語の総数、単語ベクトル、出力ラベル、隠れ層の次元、Paddingの数値は固定\n",
        "  vocab_size=len(word_id)+2\n",
        "  padding_idx=len(word_id)+1\n",
        "  output_size=4\n",
        "\n",
        "  # モデルの設定(最後に活性化関数の選択のための引数を追加しておく)\n",
        "  # RNNかCNNかを選択する\n",
        "  if model_category == \"RNN\":\n",
        "    model = RNN88(vocab_size, embd_size, output_size, hid_size, padding_idx, g)\n",
        "  elif model_category == \"CNN\":\n",
        "    # kernel_size(フィルターのサイズ)、ストライド、パディング有は固定\n",
        "    model = CNN88(vocab_size, embd_size, output_size, hid_size, padding_idx, 3, 1, 1, g)\n",
        "\n",
        "  # 損失関数、最適化関数の設定\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  # 学習率は上で設定したパラメータの中から選択される\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # データの設定においてもバッチサイズは上で設定したものが選択される\n",
        "  dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True,\n",
        "                          collate_fn=my_collate_fn)\n",
        "\n",
        "  # 学習の開始\n",
        "  # 10エポックで終了とする\n",
        "  for ep in range(10):\n",
        "    model.train()\n",
        "    for xs, ys in dataloader_train:\n",
        "      output = model(xs) \n",
        "      loss = criterion(output, ys) \n",
        "      optimizer.zero_grad()\n",
        "      loss.backward() \n",
        "      optimizer.step()\n",
        "\n",
        "  # 検証用のデータを用いて、ベストなパラメータを決定する\n",
        "  loss_valid, accuracy_valid = cal_loss_accuracy(model, criterion, dataset_valid)\n",
        "\n",
        "  # 評価に用いるのは検証データの損失とする(optunaが最小を選択するため)\n",
        "  return loss_valid "
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_9DDa4chjev",
        "outputId": "73d44763-9a79-4034-c5e0-9c44a7278de5"
      },
      "source": [
        "# create_study()を用いてパラメータの探索を行う\n",
        "study = optuna.create_study()\n",
        "# timeoutで探索を切り上げる時間を設定(今回は短めにして1000秒)\n",
        "study.optimize(objective, timeout=1000)\n",
        "\n",
        "# study.best_trialでベストパラメータと値を取得可能\n",
        "trial = study.best_trial\n",
        "# 結果の表示\n",
        "print('Best Accuracy:', trial.value)\n",
        "print('[Best Params]')\n",
        "for key, value in trial.params.items():\n",
        "  print(key ,':', value)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-07-12 00:15:57,996]\u001b[0m A new study created in memory with name: no-name-45ed2896-aedf-4f89-b0a6-198861a89589\u001b[0m\n",
            "\u001b[32m[I 2021-07-12 00:23:43,528]\u001b[0m Trial 0 finished with value: 1.1324578523635864 and parameters: {'category': 'RNN', 'embd_size': 400.0, 'hid_size': 50.0, 'learning_rate': 0.015484215232821086, 'batch_size': 4.0, 'g': 'tanh'}. Best is trial 0 with value: 1.1324578523635864.\u001b[0m\n",
            "\u001b[32m[I 2021-07-12 00:25:37,643]\u001b[0m Trial 1 finished with value: 1.0060577392578125 and parameters: {'category': 'CNN', 'embd_size': 200.0, 'hid_size': 150.0, 'learning_rate': 0.011538353895505216, 'batch_size': 12.0, 'g': 'tanh'}. Best is trial 1 with value: 1.0060577392578125.\u001b[0m\n",
            "\u001b[32m[I 2021-07-12 00:29:05,112]\u001b[0m Trial 2 finished with value: 0.9895407557487488 and parameters: {'category': 'CNN', 'embd_size': 400.0, 'hid_size': 150.0, 'learning_rate': 0.021567578797463297, 'batch_size': 12.0, 'g': 'relu'}. Best is trial 2 with value: 0.9895407557487488.\u001b[0m\n",
            "\u001b[32m[I 2021-07-12 00:32:07,079]\u001b[0m Trial 3 finished with value: 1.0736619234085083 and parameters: {'category': 'RNN', 'embd_size': 200.0, 'hid_size': 100.0, 'learning_rate': 0.006761102228393395, 'batch_size': 8.0, 'g': 'tanh'}. Best is trial 2 with value: 0.9895407557487488.\u001b[0m\n",
            "\u001b[32m[I 2021-07-12 00:35:36,318]\u001b[0m Trial 4 finished with value: 1.0693291425704956 and parameters: {'category': 'RNN', 'embd_size': 200.0, 'hid_size': 200.0, 'learning_rate': 0.006763173787438854, 'batch_size': 12.0, 'g': 'tanh'}. Best is trial 2 with value: 0.9895407557487488.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Accuracy: 0.9895407557487488\n",
            "[Best Params]\n",
            "category : CNN\n",
            "embd_size : 400.0\n",
            "hid_size : 150.0\n",
            "learning_rate : 0.021567578797463297\n",
            "batch_size : 12.0\n",
            "g : relu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-gKxufP1TYI",
        "outputId": "18118d7e-3a26-4e53-ca04-a096e84b0f6d"
      },
      "source": [
        "# 求めたパラメータで再度学習を行い評価データの正解率を求める\n",
        "# 単語の総数、単語ベクトル、出力ラベル、隠れ層の次元、Paddingの数値は固定\n",
        "vocab_size=len(word_id)+2\n",
        "padding_idx=len(word_id)+1\n",
        "output_size=4\n",
        "\n",
        "# ベストパラメータを取得\n",
        "# float型になっているので、sizeなどはint型に直しておく\n",
        "model_category = trial.params['category']\n",
        "embd_size = int(trial.params['embd_size'])\n",
        "hid_size = int(trial.params['hid_size'])\n",
        "learning_rate = trial.params['learning_rate']\n",
        "batch_size = int(trial.params['batch_size'])\n",
        "g = trial.params['g']\n",
        "\n",
        "# モデルの設定(最後に活性化関数の選択のための引数を追加しておく)\n",
        "if model_category == \"RNN\":\n",
        "  model = RNN88(vocab_size, embd_size, output_size, hid_size, padding_idx, g)\n",
        "elif model_category == \"CNN\":\n",
        "  model = CNN88(vocab_size, embd_size, output_size, hid_size, padding_idx, 3, 1, 1, g)\n",
        "\n",
        "# 損失関数、最適化関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=batch_size, shuffle=True,\n",
        "                        collate_fn=my_collate_fn)\n",
        "\n",
        "# 学習の開始\n",
        "# 10エポックで終了とする\n",
        "for ep in range(10):\n",
        "  model.train()\n",
        "  for xs, ys in dataloader_train:\n",
        "    output = model(xs) \n",
        "    loss = criterion(output, ys) \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() \n",
        "    optimizer.step()\n",
        "\n",
        "# 検証用のデータを用いて、ベストなパラメータを決定する\n",
        "loss_test, accuracy_test = cal_loss_accuracy(model, criterion, dataset_test)\n",
        "print(accuracy_test)\n",
        "# モデルの確認\n",
        "print(model)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7646176815032959\n",
            "CNN88(\n",
            "  (embd): Embedding(9399, 400, padding_idx=9398)\n",
            "  (cnn): Conv2d(1, 150, kernel_size=(3, 400), stride=(1, 1), padding=(1, 0))\n",
            "  (g): ReLU()\n",
            "  (l1): Linear(in_features=150, out_features=4, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJFFV8F-7csb"
      },
      "source": [
        "# 89. 事前学習済み言語モデルからの転移学習\n",
        "# BERTを用いるためにtransformersをインストールする\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXPWWGcS8NfR"
      },
      "source": [
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer\n",
        "# BERTの入力となる単語id列に変換する必要がある(上で設定したidとは別)\n",
        "# 変換のためにBERTを読み込む(Hugging faceに登録されている？もの)\n",
        "tknz = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yETh-G-i_rDi",
        "outputId": "b33759b5-b3ad-4557-b503-e9ed8f31984a"
      },
      "source": [
        "# 与えられた単語列に対しBERT用の単語idを返す関数\n",
        "def return_bert_id(text):\n",
        "  # 問80で作成したものと同じ前処理を施す\n",
        "  table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "  words = text.translate(table).split()\n",
        "  # encode()で文を単語id列に変換する(80とは違い文をまとめて変換可能)\n",
        "  # 辞書に登録されていない単語も自動的にidに変換される(id:100)\n",
        "  ids = tknz.encode(words)\n",
        "  return ids\n",
        "\n",
        "# 確認(適当な記事見出しを選択)\n",
        "text = train['TITLE'][0]\n",
        "print(text)\n",
        "# BERTは文頭トークン([CLS])と文末トークン([SEP])がつく(idは101と102)\n",
        "print(return_bert_id(text))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Bouygues confirms improved offer for Vivendi's SFR\n",
            "[101, 100, 26064, 4725, 2906, 1111, 100, 188, 100, 102]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCpVHU1e73GR"
      },
      "source": [
        "# BERT用データセットを作成する(基本的には前と同じ)\n",
        "class MyDataset_bert(Dataset):\n",
        "  def __init__(self, xdata, ydata, return_bert_id):\n",
        "    self.data = xdata\n",
        "    self.label = ydata\n",
        "    self.return_bert_id = return_bert_id\n",
        "\n",
        "  def __len__(self):  \n",
        "    return len(self.label)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    # 上で作成したBERT用の関数を用いて単語列をidに変換し返す\n",
        "    text = self.data[idx]\n",
        "    x = self.return_bert_id(text)\n",
        "    y = self.label[idx]\n",
        "    return x,y\n",
        "\n",
        "# 同じくDataloaderの際にPaddingを行うようにする\n",
        "def my_collate_bert_fn(batch):\n",
        "  xdata, ydata = list(zip(*batch))\n",
        "  xs = list(xdata)\n",
        "  # Paddingを行うときにPaddingを行った箇所を示すマスク行列を作成する\n",
        "  # マスクはPadding以外の部分が1となるような行列である\n",
        "  xs1, xmsk = [], []\n",
        "  for k in range(len(xs)):\n",
        "    ids = xs[k]\n",
        "    xs1.append(torch.LongTensor(ids))\n",
        "    xmsk.append(torch.LongTensor([1]*len(ids)))\n",
        "  # Paddingの数値は0とする\n",
        "  xs1 = pad_sequence(xs1, batch_first=True, padding_value=0)\n",
        "  xmsk = pad_sequence(xmsk, batch_first=True, padding_value=0)\n",
        "  \n",
        "  ys = list(ydata)\n",
        "  ys1 = torch.LongTensor(ys)\n",
        "\n",
        "  return xs1, ys1, xmsk"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RrEv-sjAoL4"
      },
      "source": [
        "# ラベルに関しては必要ないが一応ここでも記述しておく\n",
        "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
        "Y_train = torch.tensor(list(map(lambda x: category_dict[x], train['CATEGORY'])))\n",
        "Y_valid = torch.tensor(list(map(lambda x: category_dict[x], valid['CATEGORY'])))\n",
        "Y_test = torch.tensor(list(map(lambda x: category_dict[x], test['CATEGORY'])))\n",
        "\n",
        "# それぞれのデータセットを作成\n",
        "dataset_bert_train = MyDataset_bert(train['TITLE'], Y_train, return_bert_id)\n",
        "dataset_bert_valid = MyDataset_bert(valid['TITLE'], Y_valid, return_bert_id)\n",
        "dataset_bert_test = MyDataset_bert(test['TITLE'], Y_test, return_bert_id)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Y0UCFQQQYpB",
        "outputId": "61cb15cd-d225-4caf-b144-2b2926db74da"
      },
      "source": [
        "# 確認\n",
        "dataloader_train = DataLoader(dataset_bert_train, batch_size=4, shuffle=True,\n",
        "                        collate_fn=my_collate_bert_fn)\n",
        "d1 = dataloader_train.__iter__()\n",
        "xs, ys, xmsk = d1.next()\n",
        "print(xs)\n",
        "print(xmsk)\n",
        "print(len(xs))\n",
        "print(ys)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[  101,   100, 21530,  6386,  1485,   125,  2370,  1822,  3075,  1104,\n",
            "           100,   188,  1963,   102],\n",
            "        [  101,  4754,   100,  5620,  1398,  1422,  4288,  7617,   100,  1335,\n",
            "          5691,   102,     0,     0],\n",
            "        [  101, 26356,   100,  2818,  6300,  1257,  1322,  1104, 13274, 18908,\n",
            "           102,     0,     0,     0],\n",
            "        [  101,   100,   122,  3461,   100,  5351,  1136, 20342,  2631,  5351,\n",
            "          1308,   102,     0,     0]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n",
            "4\n",
            "tensor([0, 2, 0, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MDYHSa8WLzv"
      },
      "source": [
        "# モデルの実装\n",
        "class Bert89(nn.Module):\n",
        "  def __init__(self, bert):\n",
        "    super(Bert89, self).__init__()\n",
        "    # bertを読み込む\n",
        "    self.bert = bert\n",
        "    # bertの出力は768次元となる\n",
        "    # またラベルは4次元で固定なのでここで指定しておく\n",
        "    self.cls = nn.Linear(768, 4)\n",
        "  \n",
        "  # 計算時の引数としてマスクを追加する\n",
        "  def forward(self,x1,x2):\n",
        "    # attention_maskでマスクを渡せる\n",
        "    bout = self.bert(input_ids=x1, attention_mask=x2)\n",
        "    bs = len(bout[0])\n",
        "    # bout[0][i][0]はバッチ内のi番目の文に\n",
        "    # 対する[CLS]の埋め込み表現\n",
        "    # これをリストで集めstackで連結する\n",
        "    h0 = [ bout[0][i][0] for i in range(bs)]\n",
        "    h0 = torch.stack(h0,dim=0)\n",
        "    # 最終的に先頭の[CLS]の埋め込み表現を用いてラベルの予測をする\n",
        "    # 今回はsoftmax関数は用いない\n",
        "    return self.cls(h0)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyOqV2VxFr_0"
      },
      "source": [
        "# 正解率と損失の計算のための関数\n",
        "def cal_loss_accuracy_bert(model, dataset):\n",
        "  # BERTはサイズが大きいのでバッチ処理を行うとメモリが不足するので、1つずつ正解を見る\n",
        "  dataloader = DataLoader(dataset, batch_size=1, shuffle=False,\n",
        "                        collate_fn=my_collate_bert_fn)\n",
        "  real_data_num, ok = 0, 0\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for xs, ys, xmsk in dataloader:\n",
        "      output = model(xs, xmsk)\n",
        "      # 正解率のみ計算する\n",
        "      ans = torch.argmax(output,dim=1).item()\n",
        "      if ans == ys:\n",
        "        ok += 1\n",
        "      real_data_num += 1\n",
        "\n",
        "    accuracy = ok / real_data_num\n",
        "  return accuracy"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK5Yzch34Mha",
        "outputId": "5fccb4ee-29c5-498c-9cec-0f68660cbcf4"
      },
      "source": [
        "# BERTを読み込む2\n",
        "bert = BertModel.from_pretrained('bert-base-cased')\n",
        "# 損失関数、モデルのインスタンス、最適化関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = Bert89(bert)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# バッチサイズは4とする\n",
        "dataloader_train = DataLoader(dataset_bert_train, batch_size=4, shuffle=True,\n",
        "                        collate_fn=my_collate_bert_fn)\n",
        "\n",
        "# 学習の開始(流れは同じ)\n",
        "# 5エポックで終了とする(時間短縮のため)\n",
        "for ep in range(5):\n",
        "  model.train()\n",
        "  for xs, ys, xmsk in dataloader_train:\n",
        "    output = model(xs, xmsk) \n",
        "    loss = criterion(output, ys) \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() \n",
        "    optimizer.step()\n",
        "\n",
        "  accuracy_train = cal_loss_accuracy_bert(model, dataset_bert_train)\n",
        "  accuracy_test = cal_loss_accuracy_bert(model, dataset_bert_test) \n",
        "  print('エポック数', ep)\n",
        "  print('[訓練データ]正解率:',accuracy_train)\n",
        "  print('[評価データ]正解率:',accuracy_test)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "エポック数 0\n",
            "[訓練データ]正解率: 0.8897113943028486\n",
            "[評価データ]正解率: 0.856071964017991\n",
            "エポック数 1\n",
            "[訓練データ]正解率: 0.9145427286356822\n",
            "[評価データ]正解率: 0.8590704647676162\n",
            "エポック数 2\n",
            "[訓練データ]正解率: 0.9347826086956522\n",
            "[評価データ]正解率: 0.8755622188905547\n",
            "エポック数 3\n",
            "[訓練データ]正解率: 0.9416229385307346\n",
            "[評価データ]正解率: 0.8650674662668666\n",
            "エポック数 4\n",
            "[訓練データ]正解率: 0.9580209895052474\n",
            "[評価データ]正解率: 0.8718140929535232\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}